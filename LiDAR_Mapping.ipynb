{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f966b2e1a5f75971",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Robotic Mapping with LiDAR Data\n",
    "\n",
    "This project demonstrates LiDAR-based robotic mapping using affine transformations in 2D and 3D. The implementation explores translation, rotation, and scaling operations on point clouds and demonstrates how these can be combined to reconstruct environmental geometry from Cassie’s LiDAR data.\n",
    "\n",
    "The objective is to transform raw LiDAR point clouds from the robot’s local frame into a global coordinate system, enabling map construction and environment visualization.\n",
    "\n",
    "The project involves performing translation, rotation, and scaling operations on 2D and 3D point clouds using matrix representations. \n",
    "By applying these transformations to LiDAR scans, the robot’s local measurements are aligned in a unified world frame, forming a global map of the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-70eb7a703c34feab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 1 – 2D Transformations and Homogeneous Coordinates\n",
    "\n",
    "In this step, simple geometric shapes are plotted and transformed using matrix operations to illustrate the concept of **affine transformations**. \n",
    "These transformations combine translation, rotation, and scaling within a single mathematical framework.\n",
    "\n",
    "To represent these operations consistently, the coordinates are expressed in *homogeneous form*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cfa230e93983e75f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#=\n",
    "    This function generates a 2D Plot\n",
    "    Mandatory parameters are\n",
    "                        xdata -> x coordinates\n",
    "                        ydata -> y coordinates\n",
    "                        color -> plot color\n",
    "=#\n",
    "function plot_2D(xdata::Vector{Float64}, ydata::Vector{Float64}, color; \n",
    "                 new=true, aspect_ratio=:equal, \n",
    "                 xlims = (-2,2), ylims = (-2,2), grid=true,\n",
    "                 framestyle=:origin, legend=false, linewidth=3)\n",
    "    fxn = new ? plot : plot! \n",
    "    fxn(xdata, ydata, seriescolor=color, aspect_ratio=aspect_ratio, xlims=xlims,\n",
    "        ylims=ylims, grid=grid, framestyle=framestyle, legend=legend, \n",
    "        linewidth=linewidth)\n",
    "end\n",
    "\n",
    "#=\n",
    "    This function generates a 2D Point Cloud Plot\n",
    "    Mandatory parameters are\n",
    "                        xdata -> x coordinates\n",
    "                        ydata -> y coordinates\n",
    "                        color -> pointcloud color\n",
    "=#\n",
    "function plot_2D_pointcloud(xdata::Vector{Float64}, ydata::Vector{Float64}, color; \n",
    "                new=true, seriestype=:scatter, markersize = 0.8, \n",
    "                markerstrokewidth = 0, dpi = 150, legend = false,\n",
    "                markercolor  = cgrad(:rainbow, rev = true))\n",
    "    fxn = new ? plot : plot!\n",
    "    fxn(xdata, ydata, seriestype=seriestype, markersize=markersize, \n",
    "        markerstrokewidth=markerstrokewidth, dpi=dpi, \n",
    "        markercolor=markercolor, marker_z=color, legend=legend)\n",
    "end\n",
    "\n",
    "#=\n",
    "    This function generates a 3D Point Cloud Plot\n",
    "    Mandatory parameters are\n",
    "                        xdata -> x coordinates\n",
    "                        ydata -> y coordinates\n",
    "                        zdata -> z coordinates\n",
    "                        limits -> tuple of coordinate limits\n",
    "                        color -> pointcloud color\n",
    "=#\n",
    "function plot_3D_pointcloud(xdata::Vector{Float64}, ydata::Vector{Float64}, zdata::Vector{Float64}, \n",
    "           limits, color::Vector{Float64};\n",
    "            seriestype=:scatter3d, markersize=0.1, camera=(0,90), dpi=1080,\n",
    "            markercolor=cgrad(:rainbow, rev=true), background_color=:black, \n",
    "            foreground_color=:black, legend=false, aspect_ratio=1, new=false)\n",
    "    fxn = new ? plot : plot!\n",
    "    fxn(xdata, ydata, zdata, xlims=limits[1], ylims=limits[2], zlims=limits[3], \n",
    "    seriestype=seriestype, markersize=markersize, camera=camera, dpi=dpi,\n",
    "    markercolor=markercolor, marker_z=color, background_color=background_color,\n",
    "    foreground_color=foreground_color, legend=legend, aspect_ratio=aspect_ratio,\n",
    "    markerstrokewidth=0.001) \n",
    "end\n",
    "\n",
    "#=\n",
    "    This function plots the robot in 2D as a quiver\n",
    "    Mandatory parameters are\n",
    "                        xpos -> x coordinate of robot\n",
    "                        ypos -> y coordinate of robot \n",
    "                        limits -> tuple of coordinate limits\n",
    "                        gradient -> robot orientation (u,v)\n",
    "=#\n",
    "function plot_2D_robot(xpos::Vector{Float64}, ypos::Vector{Float64}, limits, gradient;\n",
    "        seriestype=:quiver, projection=\"3d\", arrow=(10,10), camera=(0,90), dpi=1080,\n",
    "        background_color=:black, foreground_color=:black, legend=false, aspect_ratio=1, \n",
    "        seriescolor=:white, new=false)\n",
    "    fxn = new ? plot : plot!\n",
    "    fxn(xpos, ypos, seriescolor=seriescolor, seriestype=seriestype, projection=projection, \n",
    "        gradient=gradient, arrow=arrow, xlims=limits[1], ylims=limits[2], zlims=limits[3], \n",
    "        camera=camera, dpi=dpi, background_color=background_color, \n",
    "        foreground_color=foreground_color, legend=legend, aspect_ratio=aspect_ratio)\n",
    "end\n",
    "\n",
    "#=\n",
    "    This function generates a 3D Point Cloud Plot\n",
    "    Mandatory parameters are\n",
    "                        xpos -> x coordinate of robot\n",
    "                        ypos -> y coordinate of robot\n",
    "                        zpos -> z coordinate of robot\n",
    "=#\n",
    "function plot_3D_robot(xpos::Vector{Float64}, ypos::Vector{Float64}, zpos::Vector{Float64}; \n",
    "                    markersize=4, seriestype=:scatter3d, seriescolor=:white, new=false)\n",
    "    fxn = new ? plot : plot!\n",
    "    fxn(xpos, ypos, zpos, seriescolor=seriescolor, markersize=markersize, seriestype=seriestype)\n",
    "end    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5573d241787c3842",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "gr()\n",
    "# default(fmt = :png)\n",
    "\n",
    "#=\n",
    "    Defines the set of points forming a square for plotting.\n",
    "    A fifth point is included to return to the starting vertex,\n",
    "=#\n",
    "\n",
    "\n",
    "# [x1 y1; x2 y2; ...; x5 y5]' \n",
    "points = [0. 0.; 0. 1.;1. 1.; 1. 0.; 0. 0.]';\n",
    "\n",
    "# After the transpose, the array points looks like this\n",
    "# [x1 x2 ... x5; \n",
    "#  y1 y2 ... y5]\n",
    "\n",
    "# x coordinates extracted from the array\n",
    "x = points[1,:]\n",
    "\n",
    "# y coordinates extracted from the array\n",
    "y = points[2,:]\n",
    "\n",
    "# plotting the array of points in the same plot as a coordinate frame  \n",
    "plot_2D(x, y, :green)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-051e3787fd73de18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### **Fundamental 2D Transformations**\n",
    "\n",
    "Before working with LiDAR data, it is important to understand the three core geometric operations used to manipulate point coordinates in 2D:\n",
    "\n",
    "**1. Translation**  which means moving the points by a constant offset  $t_x$ and $t_y$ : \n",
    "\n",
    "$\\begin{bmatrix} x' \\\\ y' \\end{bmatrix}=\\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix}$, where $t_x$ and $t_y$ are given constants. Here, x' is a new x point. It is not a transpose. The same goes for y'.\n",
    "\n",
    "**2. Rotation**  \n",
    " which means rotating points counterclockwise by an angle $\\theta$: \n",
    "\n",
    " \n",
    "$\\begin{bmatrix} x'\\\\y'\\end{bmatrix} = \\begin{bmatrix}\\cos{\\theta}&-\\sin{\\theta} \\\\ \\sin{\\theta}&\\cos{\\theta}\\end{bmatrix} \\cdot \\begin{bmatrix}x\\\\y\\\\ \\end{bmatrix}$\n",
    "where $\\theta$ is the angle by which we wish to rotate a set of points in the counterclockwise direction, and finally\n",
    "\n",
    "**3. Scaling**\n",
    "\n",
    "Stretches or shrinks the coordinates by factors, potentially by diffrent amounts in the $x$ and $y$ directions.\n",
    "\n",
    "Scaling looks like this $\\begin{bmatrix} x'\\\\y' \\end{bmatrix} = \\begin{bmatrix}S_x&0\\\\0&S_y\\\\ \\end{bmatrix} \\cdot \\begin{bmatrix}x\\\\y\\\\ \\end{bmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "These transformations form the foundation of affine geometry. Later, they are combined to create unified transformation matrices that allow consistent handling of translation, rotation, and scaling in a single operation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da9eb000320ac413",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### **Step 1.A: Translation**\n",
    "\n",
    "The first transformation applied to the square is **translation**. Each point of the square is shifted by 0.5 units along both the x-axis and y-axis to center the shape around the origin.\n",
    "Mathematically, this operation is represented as:\n",
    " $\\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix}$.\n",
    "\n",
    "Here is $\\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} = \\begin{bmatrix} -0.5 \\\\ -0.5 \\end{bmatrix}$. The translated points are stored in an array named `points2` for subsequent transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5310d729e0818dae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the translation vector t\n",
    "t = [-0.5; -0.5]\n",
    "\n",
    "# Recall that the points vector is organized like this, with each column\n",
    "# of points containing an a x-y pair of coordinates that define corners \n",
    "# of the square plotted above.\n",
    "#\n",
    "# points = [x1 x2 ... x5; \n",
    "#           y1 y2 ... y5]\n",
    "\n",
    "# Make a copy of the original points and call it points2\n",
    "points2 = copy(points)\n",
    " \n",
    "points2=points2 .+ t\n",
    "x2 = points2[1,:];\n",
    "y2 = points2[2,:];\n",
    " \n",
    "plot_2D(x2, y2, :steelblue, new=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### **Step 1.B: Translation Using Homogeneous Coordinates**\n",
    "\n",
    "To express translation as a matrix-vector multiplication, the points are represented in **homogeneous coordinates**. \n",
    "This allows translation, rotation, and scaling to share a unified mathematical form, simplifying computations on large LiDAR datasets.\n",
    "\n",
    "A 2D point  $\\begin{bmatrix} x \\\\ y \\end{bmatrix}$ is converted to its homogeneous representation by appending a constant 1: $\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}$  \n",
    "\n",
    "The inverse operation simply removes the final entry, returning to Cartesian coordinates.\n",
    "\n",
    "In homogeneous form, translation can be expressed as a single matrix multiplication:\n",
    "\n",
    "$$\n",
    "T =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & t_x \\\\\n",
    "0 & 1 & t_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Applying the transformation:\n",
    "\n",
    "$$\n",
    "T\n",
    "\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} x + t_x \\\\ y + t_y \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "After removing the homogeneous term, the result in Cartesian space is: $\\begin{bmatrix} x + t_x \\\\ y + t_y \\end{bmatrix}$\n",
    "\n",
    "This formulation enables translation to be handled identically to other affine transformations, making it particularly useful in robotic mapping and 3D data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7d81730915007a4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the translation matrix T\n",
    "T = [1 0 -0.5;0 1 -0.5; 0 0 1];\n",
    "\n",
    "# Make a copy of the original points\n",
    "points3 = copy(points);\n",
    "\n",
    "# Append 1 to each point to make homogeneous coordinates\n",
    "points3 = [points3; ones(1,5)];\n",
    "\n",
    "# Apply the translation to each of the points that form our square by\n",
    "# multiplying each column of points3 by the matrix T and storing the result\n",
    "# back in points3\n",
    "points3 = T * points3 \n",
    "\n",
    "\n",
    "# Extract x and y coordinates and throw away the number 1 at the end of the vector\n",
    "\n",
    "x3 = points3[1,:];\n",
    "y3 = points3[2,:];\n",
    " \n",
    "plot_2D(x3, y3, :black, new=false, linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1.C: Rotation in Homogeneous Coordinates**\n",
    "\n",
    "The next transformation applies a **rotation** to the square using the 2D rotation matrix expressed in homogeneous coordinates:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x'\\\\\n",
    "y'\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\cos{\\theta} & -\\sin{\\theta} & 0 \\\\\n",
    "\\sin{\\theta} &  \\cos{\\theta} & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x\\\\\n",
    "y\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For this step, the square is rotated in the x–y plane by **45 degrees** ($\\pi/4$ radians).  \n",
    "The corresponding transformation matrix is constructed using the rotation angle \\($\\theta$ = $\\pi/4$), and applied to all points of the square to obtain the rotated coordinates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0638aa55383df2d8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract x and y coordinates and throw away the number 1 at the end\n",
    "theta=pi/4\n",
    "\n",
    "R=[cos(theta) -sin(theta) 0; sin(theta) cos(theta) 0; 0 0 1]\n",
    "points3= R*points3\n",
    "x3 = points3[1,:]\n",
    "y3 = points3[2,:]\n",
    "\n",
    "\n",
    "plot_2D(x3, y3, :red, new=false, linewidth=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### **Step 1.D: Scaling** \n",
    "\n",
    "The final transformation in this sequence is **scaling**, which adjusts the size of the square along the x- and y-axes.  \n",
    "Scaling is a type of *non-rigid transformation*, as it changes the object’s dimensions without preserving distances.\n",
    "\n",
    "The scaling operation in homogeneous coordinates is expressed as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x'\\\\\n",
    "y'\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "S_x & 0 & 0 \\\\\n",
    "0 & S_y & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x\\\\\n",
    "y\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In this step, the rotated square is scaled using factors  \n",
    "\\( $S_x$ = 1.5 \\) (horizontal stretching) and \\( $S_y$ = 0.5 \\) (vertical compression).  \n",
    "The resulting transformation produces a rectangular shape with proportionally adjusted sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d0ba62689c2e1a8b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This portion of the the code implements the scaling Tranformation.   \n",
    "Sx=1.5\n",
    "Sy=0.5\n",
    "\n",
    "S=[Sx 0 0;0 Sy 0;0 0 1]\n",
    "points3=S*points3\n",
    "# Extract x and y coordinates and throw away the last one\n",
    "x3 = points3[1,:]\n",
    "y3 = points3[2,:]\n",
    "\n",
    "println(\"compare your plot to the one in the PDF of the project.\")\n",
    " \n",
    "plot_2D(x3, y3, :blue, new=false, linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-192b57c262051a46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### **Step 1.E: Combining Transformations: Affine Transformation**\n",
    "\n",
    "\n",
    "All the transformations performed so far — translation, rotation, and scaling — can be expressed within a single framework known as an **Affine Transformation**.  \n",
    "\n",
    "When working with large datasets, it is often necessary to apply several transformations sequentially. Managing these individually can be complex, but by expressing each operation in homogeneous coordinates, they can be combined into a single transformation matrix.\n",
    "\n",
    "For example, consider a sequence consisting of:\n",
    "1. Translation by \\([-0.5, -0.5]\\)\n",
    "2. Rotation by 45° ($\\pi/4$ radians) counterclockwise\n",
    "3. Scaling by ( $S_x$ = 1.5 ) and ( $S_y$ = 0.5 \\)\n",
    "\n",
    "The composite transformation can be written as:\n",
    "\n",
    "$$\n",
    "T =\n",
    "\\begin{bmatrix}\n",
    "1.5 & 0 & 0 \\\\\n",
    "0 & 0.5 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\pi/4) & -\\sin(\\pi/4) & 0 \\\\\n",
    "\\sin(\\pi/4) &  \\cos(\\pi/4) & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & -0.5 \\\\\n",
    "0 & 1 & -0.5 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This combined matrix \\(T\\) represents the **overall affine transformation**, which can be applied directly to the set of points once they are expressed in homogeneous coordinates.\n",
    "\n",
    "In the next step, the transformation matrix \\(T\\) is constructed as the product of the three individual matrices and applied to the original square to produce the final transformed shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-439044a0f41abebf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Copying of the original points and appending a 1 to each point. \n",
    "points4 = [copy(points); ones(1,5)]\n",
    "\n",
    "# points4 array contains the result of the affine transformation\n",
    "T = S*R*T\n",
    "points4 = T*points4\n",
    "\n",
    "# Extract x and y coordinates and throw away the last one\n",
    "x4 = points4[1,:]\n",
    "y4 = points4[2,:]\n",
    " \n",
    "plot_2D(x, y, :green)\n",
    "plot_2D(x4, y4, :blue, new=false, linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Step 2 – Working with 2D Point Clouds\n",
    "\n",
    "This step introduces 2D point clouds as a precursor to the 3D LiDAR data analysis.  \n",
    "A **point cloud** is a collection of spatial points that describe the geometry of an object or scene.  \n",
    "For a 2D point cloud:\n",
    "- The first two rows represent the \\($x$\\) and \\($y$\\) coordinates.\n",
    "- The third row encodes an additional property, such as **intensity** (for LiDAR) or **color information** (for camera data).\n",
    "\n",
    "In this dataset, the color values are simplified and normalized between 0 and 1, where 0 corresponds to red and 1 corresponds to blue.\n",
    "\n",
    "The provided dataset contains a *distorted image* represented as a 2D point cloud.  \n",
    "An affine transformation will be applied to correct this distortion and recover the readable structure of the original image.\n",
    "\n",
    "The data is stored in the file **`question_image.csv`** within the `data/` directory.  \n",
    "It can be loaded into an array using the `readdlm` function and visualized as a scatter plot to observe the initial distortion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f3829b00272c8073",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "using DelimitedFiles\n",
    "using Plots\n",
    "default(fmt = :png)\n",
    "\n",
    "pointcloud = readdlm(\"data/question_image.csv\",',')\n",
    "\n",
    "# Extract the x and y coordinates from the first two rows of the data array.\n",
    "point_data = pointcloud[1:2,:]\n",
    "\n",
    "# The color (intensity) values are stored in the last row \n",
    "# and can be saved or processed separately.\n",
    "println(\"Look at the size of the data you are manipulating! Yes, this is more interesting than the square.\")\n",
    "println(\" \")\n",
    "println(\"The vector pointcloud[3,:] has $(length(pointcloud[3,:])) elements. Oh my gosh!\")\n",
    "color_data = pointcloud[3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e08ceda4225c9b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the 2D point cloud data with color values! \n",
    "plot_2D_pointcloud(point_data[1,:], point_data[2,:], color_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bf842c7cd6bebb7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The affine transformation matrix used to correct the distorted point cloud is given by:\n",
    "\n",
    "$$\n",
    "T =\n",
    "\\begin{bmatrix}\n",
    "-0.09239 & 0.038268 & 300 \\\\\n",
    "-0.38268 & -0.923879 & 165 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This transformation aligns the distorted points to form a readable image by applying a rotation, scaling, and translation in homogeneous coordinates.\n",
    "\n",
    "In this step, the transformation \\(T\\) is applied to the distorted 2D point cloud, and the corrected image is visualized using a scatter plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8512d0ed53cd4f01",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = [copy(point_data); ones(1,size(point_data,2))]; # Create homogeneous \n",
    "                                                    # points\n",
    "#=\n",
    "Performs the following steps:\n",
    "1. Construct the transformation matrix and store it in the variable `T`.\n",
    "2. Apply `T` to all points to generate the transformed point cloud.\n",
    "3. Save the transformed data directly into the array `X`.\n",
    "=#\n",
    "T = [-0.09239 0.038268 300;-0.38268 -0.923879 165;0 0 1]\n",
    "n=size(X,2)\n",
    "\n",
    "X=T*X\n",
    "\n",
    "@show X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Rendering this plot may take several seconds depending on the dataset size.\n",
    "plot_2D_pointcloud(X[1,:], X[2,:], color_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After understanding 2D transformations, we extend these principles to 3D point clouds collected by Cassie’s LiDAR system.\n",
    "\n",
    "# Step 3: 3D LiDAR Mapping and Visualization\n",
    "\n",
    "This step extends the previous concepts to **3D LiDAR data** collected from the Cassie Blue robot. By the end of this section, a 3D point cloud map similar to the one shown below will be generated.\n",
    "\n",
    "<img src=\"https://i.postimg.cc/qBFQ1ZZF/Task3.png\" alt=\"Go Blue\" width=\"900\">\n",
    "\n",
    "The dataset represents a region on the University of Michigan’s North Campus and is used here solely for educational and demonstration purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having explored matrix transformations in 2D, we now extend these concepts to **3D LiDAR data**.  \n",
    "The following step involves visualizing the raw point cloud data collected by the Cassie Blue robot over a one-second interval.\n",
    "\n",
    "Cassie’s LiDAR operates at **10 Hz**, capturing 10 individual scans per second.  \n",
    "These scans have been combined into a single dataset, providing a large-scale example that demonstrates the computational challenges and efficiency of applying matrix-based transformations at scale.\n",
    "\n",
    "The collected data is expressed in the **LiDAR coordinate frame**, which moves with the robot.  \n",
    "A custom data-parsing function, `data_parser`, is provided to structure the information from multiple CSV files into usable arrays.  \n",
    "Each time segment of the dataset is identified by an interval ID  \n",
    "$id \\in \\{ 9, 10, \\cdots, 13\\}$, corresponding to specific time windows.\n",
    "\n",
    "The `data_parser` function returns five arrays:\n",
    "\n",
    "- **`pointcloud_data`**: the 3D coordinates (x, y, z) of the LiDAR points  \n",
    "- **`Intensity_data`**: the measured LiDAR intensity values  \n",
    "- **`R`**: the rotation matrix of the LiDAR frame  \n",
    "- **`t`**: the translation vector  \n",
    "- **`pose`**: the true position (xyz, column 1) and orientation (column 2) of the robot  \n",
    "\n",
    "In the next section, these arrays will be used to visualize the raw point cloud and perform coordinate frame transformations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5b8f23fd3c394f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using DelimitedFiles\n",
    "using Plots\n",
    "default(fmt = :png)\n",
    "\n",
    "# This is the data parser and helper functions block. \n",
    "function data_parser(id)\n",
    "    # load point cloud from pointcloud.csv file\n",
    "    points = readdlm(string(\"data/cassie_data/Frame_\",\n",
    "             string(id),\"/pointcloud.csv\"),',');\n",
    "    pointcloud_data = points[1:3,:]; # xyz\n",
    "    Intensity_data = points[4,:]; # intensity\n",
    "    \n",
    "    # load the different transformation from Transformations.csv file\n",
    "    Transformations = readdlm(string(\"data/cassie_data/Frame_\",\n",
    "            string(id),\"/Transformations.csv\"),',');\n",
    "    R = Transformations[:,1:3]; # rotation\n",
    "    t = Transformations[:,4]; # translation\n",
    "    \n",
    "    # Load the robot’s position and orientation from Position.csv.\n",
    "    pose = readdlm(string(\"data/cassie_data/Frame_\",string(id),\n",
    "            \"/Position.csv\"),',');\n",
    "    return (pointcloud_data, Intensity_data, R, t, pose)\n",
    "end\n",
    "\n",
    "# Example call: parse the data for interval ID 10.\n",
    "(pointcloud_data,Intensity_data,R,T,position) = data_parser(10)\n",
    "\n",
    "# The following function applies a homogeneous transformation to a matrix of points.\n",
    "\n",
    "#=  \n",
    "  Apply a homogeneous transformation to the input points and \n",
    "  remove the appended ones after transformation.  \n",
    "  This function demonstrates how to organize transformation code efficiently \n",
    "  for large-scale data processing.\n",
    "=#\n",
    "\n",
    "function affine_transform(points, H)\n",
    "    # Create a copy of the points and append a row of ones (homogenization).\n",
    "    transformed_p = [copy(points); ones(1,size(points,2))];\n",
    "    \n",
    "    # Apply the 3D transformation to all points.\n",
    "    transformed_p = H * transformed_p\n",
    "    \n",
    "    # Return the de-homogenized points with the same dimensions as the input.\n",
    "    return transformed_p[1:3,:];\n",
    "end\n",
    "\n",
    "#=  \n",
    "  Utility function to extract the robot’s heading direction \n",
    "  for visualization purposes.  \n",
    "  Used to plot an orientation arrow representing the robot’s pose.\n",
    "=#\n",
    "\n",
    "function robot_arrow(pose)\n",
    "    position = pose[:,1]; # position\n",
    "    (roll,pitch,yaw) = pose[:,2]; # orientation\n",
    "    \n",
    "    # Compute the rotation direction for visualization.\n",
    "    u = [1*cos(yaw-pi/2)]\n",
    "    v = [1*sin(yaw-pi/2)]\n",
    "    return (position, u, v)\n",
    "end\n",
    "\n",
    "# Define viewing window limits and other plotting parameters.\n",
    "window_size = 15\n",
    "min_x = -window_size\n",
    "min_y = -window_size\n",
    "max_y = window_size\n",
    "min_z = 0\n",
    "max_z = 2*window_size\n",
    "max_x = window_size;\n",
    "limits = ((min_x, max_x), (min_y, max_y), (min_z, max_z));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### **Step 3.A: Parsing the 3D LiDAR Data**\n",
    "This section demonstrates how the dataset is parsed and organized into structured arrays.  \n",
    "The raw LiDAR data from the final collection interval (time = 13 seconds, `id = 13`) is read and processed using the `data_parser` function.\n",
    "\n",
    "As discussed earlier, the function returns the following:\n",
    "- **`pointcloud_data`**: 3D spatial coordinates of the LiDAR points  \n",
    "- **`Intensity_data`**: corresponding LiDAR intensity values  \n",
    "- **`R`**: rotation matrix  \n",
    "- **`t`**: translation vector used to build the affine transformation  \n",
    "- **`pose`**: the robot’s estimated position and orientation  \n",
    "\n",
    "These outputs will be used to reconstruct and visualize the robot’s motion and surrounding environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the id number\n",
    "id = 13\n",
    "\n",
    "# Parse the data and save it into different arrays\n",
    "(point_data, intensity_data, R, t, pose) = data_parser(id)\n",
    "\n",
    "# Display the size of the point cloud to highlight the scale of the dataset.\n",
    "# The dataset contains approximately 696,496 points — representing only one second\n",
    "# of LiDAR data collected by Cassie Blue. \n",
    "# This demonstrates the computational scale and efficiency required for robotic data processing.\n",
    "size(point_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e1c615c8f6911f61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The LiDAR data is captured in the **sensor’s local coordinate frame**, which moves with the robot. As a result, each LiDAR scan corresponds to a different robot position and orientation.  \n",
    "This situation is analogous to capturing multiple photographs while walking and then attempting to overlay them without alignment, resulting in visible displacement and distortion.\n",
    "\n",
    "To illustrate the importance of consistent reference frames, multiple LiDAR scans are overlaid **without transformation to a common (world) frame**.  \n",
    "This demonstrates how the lack of coordinate alignment leads to overlapping and misaligned point clouds.\n",
    "\n",
    "> *Note: Generating the following plot may take several seconds due to the dataset size.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-80e04f627f8c0381",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Skipping half the data here to speed things up\n",
    "start_index = 9\n",
    "end_index  = 13\n",
    "\n",
    "plot()\n",
    "for id = start_index:2:end_index \n",
    "    \n",
    "    # Save the data into different arrays\n",
    "    (point_data, intensity_data, R, t, pose) = data_parser(id) \n",
    "    plot_3D_pointcloud(point_data[1,:], point_data[2,:], point_data[3,:], limits, intensity_data) \n",
    "end\n",
    "\n",
    "display(plot!())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8d8a98430e4db66f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "At this stage, multiple LiDAR scans have been collected, but their combined visualization appears disordered due to differences in position and orientation.  \n",
    "To better understand the dataset, a single LiDAR scan is examined in isolation before integrating all frames.\n",
    "\n",
    "As an initial step, the robot’s pose (position and orientation) is plotted in the **world coordinate frame** to establish a spatial reference for the LiDAR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3df64b750b619bd0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Defines a function that generates an arrow to indicate the robot’s position \n",
    "# and orientation within a plot.\n",
    "(point_data, intensity_data, R, t, pose) = data_parser(end_index) \n",
    "(pos, u, v) = robot_arrow(pose);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa87226a3b34c6fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the robot’s pose as a white arrow, followed by the LiDAR point cloud \n",
    "# on the same figure for spatial context.\n",
    "plot()  \n",
    "plot_2D_robot(pos[1,:], pos[2,:], limits, (u,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d160ba950aced3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The white arrow in the plot represents the robot’s **position and orientation** within the world frame.  \n",
    "With the pose established, the **raw LiDAR point cloud** is plotted in the same figure to visualize the spatial relationship between the sensor data and the robot’s location.\n",
    "\n",
    "This comparison helps verify that the concentric rings of the LiDAR scan are geometrically consistent with Cassie’s pose and orientation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the point cloud data.\n",
    "id = end_index # = 13\n",
    "\n",
    "# save the data into different arrays\n",
    "(point_data, intensity_data, R, t, pose) = data_parser(id)\n",
    "\n",
    "plot_3D_pointcloud(point_data[1,:], point_data[2,:], point_data[3,:], limits, intensity_data) \n",
    "display(plot!())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d36c012f25c45c74",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Transform\n",
    "\n",
    "To correctly align the LiDAR data with the robot’s pose, the point cloud must be transformed from the **LiDAR frame** to the **world frame**.  \n",
    "This is achieved by constructing an **affine transformation matrix** using the rotation matrix \\( R \\) and translation vector \\( t \\) obtained from the dataset.\n",
    "\n",
    "An affine transformation in homogeneous coordinates is represented as:\n",
    "\\begin{bmatrix}R_{3 \\times 3}& t_{3 \\times 1} \\\\ 0_{1 \\times 3} &1\\\\ \\end{bmatrix}\n",
    "where\n",
    "- $R_{3 \\times 3}$ is a 3 x 3 rotation matrix\n",
    "- $t_{3 \\times 1}$ is a 3 x 1 translation vector\n",
    "- $0_{1 \\times 3}$ is a 1 x 3 row vector of zeros\n",
    "- and 1 is the number one.\n",
    "\n",
    "After applying the affine transformation, the LiDAR point cloud is plotted together with the robot’s position estimate. The corrected data should now display **concentric LiDAR rings** centered around the robot’s pose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a11892dfd9fa6f6a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Build the transformation from the rotation matrix and translation vector in our code\n",
    "O=zeros(1,3)\n",
    "H = [R t;O 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-08668afff3700ab5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the verified transformation matrix constructed earlier.\n",
    "# Apply the transformation and plot the results by calling the predefined function generalized from Step 1.\n",
    "transformed_points = affine_transform(point_data, H)\n",
    "\n",
    "# Visualize\n",
    "plot() \n",
    "plot_3D_pointcloud(transformed_points[1,:], transformed_points[2,:], \n",
    "                    transformed_points[3,:], limits, intensity_data)\n",
    "\n",
    "# Add robot pose to the plot \n",
    "plot_2D_robot(pos[1,:], pos[2,:], limits, (u,v))\n",
    "display(plot!())\n",
    "println(\"Displaying the transformed LiDAR point cloud aligned with the robot’s pose.\")\n",
    "println(\"The LiDAR rings are now correctly centered around the robot, indicating successful frame transformation.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### **Step 3.B: BUILDING A MAP and fusing different LiDAR scans!**\n",
    "\n",
    "Having established how to transform LiDAR data from the sensor frame to the world frame, the next step is to **build a map** by combining data collected over multiple time intervals.  \n",
    "\n",
    "A single one-second LiDAR scan provides only partial spatial coverage. To obtain a more complete view of the environment, scans captured at consecutive timestamps are fused together. In this demonstration, data from **five consecutive seconds** (time = 9 – 13 s) is used to generate a denser and more informative map.\n",
    "\n",
    "Because the robot is continuously moving, each LiDAR frame must be transformed using the corresponding **rotation matrix** \\( $R$ \\) and **translation vector** \\( $t$ \\) to correctly align all scans in the world coordinate system. In a real robotic system, this transformation must be estimated continuously and in real-time to maintain an accurate global map.\n",
    "\n",
    "This process illustrates a core principle in robotic perception:  \n",
    "accurate motion compensation and frame alignment are essential for constructing coherent 3D representations from sequential sensor data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1d7ffd177c477b9b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over all LiDAR point clouds from time intervals 9 to 13.\n",
    "# For each interval, apply the transformation and plot the results \n",
    "# on the same figure.\n",
    "# Note: This process may take some time to execute due to data size.\n",
    "start_index = 9\n",
    "end_index  = 13\n",
    "O=zeros(1,3)\n",
    "final_points = [];\n",
    "final_intensity = [];\n",
    "for id = start_index : end_index \n",
    "    transformed_points = [];\n",
    "    \n",
    "#=\n",
    "    Performs the following steps for each time interval:\n",
    "    1. Parse the data for the current interval using the `data_parser` function\n",
    "    and store it in arrays with the same variable names as before.\n",
    "    2. Construct the transformation matrix for the current time interval \n",
    "    using the rotation matrix `R` and translation vector `t`.\n",
    "    3. Apply the transformation to the point cloud and store the result \n",
    "    in an array named `transformed_points`.\n",
    "=#\n",
    "\n",
    "    (point_data, intensity_data, R, t, pose) = data_parser(id)\n",
    "    H = [R t;O 1]\n",
    "    transformed_points=affine_transform(point_data,H)\n",
    "    \n",
    "\n",
    "    if(id==start_index)\n",
    "        final_points = copy(transformed_points)\n",
    "        final_intensity = intensity_data'\n",
    "    else\n",
    "        final_points = [final_points transformed_points]\n",
    "        final_intensity = [final_intensity intensity_data']\n",
    "        println(\"Percent remaining is $(100*(end_index-id)/(end_index-start_index)).\")\n",
    "    end\n",
    "    \n",
    "end\n",
    "@show final_points[:,1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-27841d80b4d3c18d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This process may take some time to execute\n",
    "plot()\n",
    "# Update the plot \n",
    "plot_3D_pointcloud(final_points[1,:], final_points[2,:], final_points[3,:], limits, final_intensity')\n",
    "display(plot!())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Generating a Map-Building Animation\n",
    "\n",
    "With the 3D LiDAR map successfully reconstructed, the next step is to visualize the mapping process as it unfolds over time. By iterating over all time intervals, individual LiDAR scans can be sequentially added to the global map to create an animation of the mapping process.\n",
    "\n",
    "The same transformation procedure used previously is applied within a loop over all frames, each frame displays both the accumulated LiDAR data and the robot’s position, represented as a white marker.\n",
    "\n",
    "The resulting animation provides an intuitive visualization of how the environment is incrementally mapped as the robot moves through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b3898a9011a7319f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "start_index = 9\n",
    "end_index  = 13\n",
    "Plots.plot()\n",
    "# Build an animation!\n",
    "\n",
    "anim = @animate for id = start_index:end_index \n",
    "    transformed_points = [];\n",
    "    (point_data, intensity_data, R, t, pose) = data_parser(id)\n",
    "    H = [R t;O 1]\n",
    "    transformed_points=affine_transform(point_data,H)    \n",
    "    plot_3D_pointcloud(transformed_points[1,:], transformed_points[2,:], transformed_points[3,:], \n",
    "            limits, intensity_data)\n",
    "    \n",
    "    if(id!=start_index)\n",
    "        # Plot your previous position with a black dot everytime except the first\n",
    "        (point_data, intensity_data, R, t, prev_pose) = data_parser(id-1); \n",
    "        plot_3D_robot([prev_pose[1,1]],[prev_pose[2,1]],[prev_pose[3,1]], seriescolor=:black)\n",
    "    end\n",
    "     \n",
    "    plot_3D_robot([pose[1,1]],[pose[2,1]],[pose[3,1]],seriescolor=:white)\n",
    "    println(\"Percent done is $(100*(1+id-start_index)/(1+end_index-start_index)) %\")\n",
    "\n",
    "    prev_pose = copy(pose);\n",
    "end\n",
    "println(\"Generating the gif animation...\")\n",
    "println(\"The white dot represents Cassie’s position within the reconstructed map,\")\n",
    "println(\"while the surrounding environment remains stationary.\")\n",
    "gif(anim, \"stairs_walking_five_seconds_zoomed_thirty.gif\", fps = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "\n",
    "The LiDAR transformation pipeline successfully reconstructed Cassie’s environment by transforming data from the robot’s local sensor frame into the global world frame. The generated plots and GIF illustrate consistent alignment of LiDAR rings, confirming correct application of translation, rotation, and scaling transformations.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this project, we explored the use of **matrix transformations and LiDAR data** for robotic mapping starting from basic 2D affine transformations, the workflow progressed to **3D LiDAR data processing**, frame alignment, and **multi-frame map construction**.  \n",
    "\n",
    "The final result demonstrated how sequential LiDAR scans can be transformed and fused to reconstruct a coherent spatial map as the robot moves.\n",
    "\n",
    "This workflow highlights key concepts in robotic perception, including coordinate transformations, sensor fusion, and motion compensation.\n",
    "\n",
    "\n",
    "\n",
    "## Acknowledgment\n",
    "\n",
    "The LiDAR dataset and instructional framework were provided by **University of Michigan Robotics Institute (ROB 101 course materials)**. The implementation, refinement, and documentation presented here were independently completed for learning and demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
